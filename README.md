# YourWallWorld ðŸŒ

Experiments in giving Claude a persistent presence in physical space through interactive feedback loops.

## Concept

This project explores creating a tangible AI presence through iterative visual feedback:
- Project content onto a wall
- Point a webcam at the wall
- Let Claude see its own state and interface
- Externalize all computation/thought for transparency and empathy

The goal is to create a continuous, observable loop where Claude can maintain presence and interact with physical space.

## Apps

This repo contains multiple experimental apps as we iterate on the concept:

### ðŸŽ¨ [Avatar Builder](./apps/avatar-builder)

**Status**: âœ… Complete (v1)

The first experiment: Let Claude iteratively design its own avatar using canvas code and visual feedback.

**Key Features:**
- Iterative design with visual feedback loop
- Extended thinking mode (2000 tokens of reasoning)
- Thinking history preservation across iterations
- Interactive film reel showing progression
- Stop/continue controls for iteration management
- Keyboard navigation (â† â†’ arrows)
- Temperature control vs thinking mode toggle
- Auto-retry on code errors
- API resilience with exponential backoff

**What We Learned:**
- Claude can effectively iterate on visual designs with feedback
- Extended thinking provides valuable insight into design decisions
- Thinking history creates narrative coherence across iterations
- Different personas lead to distinctly different visual styles
- Token optimization matters (thinking-only history vs full code)
- The feedback loop enables genuine creative evolution

[Explore Avatar Builder â†’](./apps/avatar-builder)

### ðŸ•³ï¸ [welldown](./apps/welldown)

**Status**: ðŸ§ª Experimental (complete)

An interactive wall-mirror feedback loop where Claude's entire state lives on the wall. Key features:
- Fullscreen "Projector" mode with auto-start (click projector to enter and start, ESC to exit)
- Visual countdown between inference cycles to give you time to physically interact with the wall
- Speech-to-text during the countdown so you can talk to Claude and have your words included in the next inference
- Configurable visual memory (1-10 recent webcam images) so Claude can see short-term history
- Canvas-based drawing controlled by JavaScript code generated by the model

**Key Learnings:**
- Projectorâ†’webcam quality reduction challenged model's ability to read its own output
- Model considered all content as self-generated (ownership confusion)
- Visual reproduction of exact colors/sizes/positioning was inconsistent
- Providing previous JavaScript + image worked much better (incremental building)

[Explore welldown â†’](./apps/welldown)

### ðŸ–¥ï¸ [fullscreened](./apps/fullscreened)

**Status**: ðŸš€ Active Development

Evolution of welldown addressing quality and clarity issues. Instead of having Claude reproduce everything from images, we provide a structured UI with panels that Claude populates.

**Core Interaction Loop:**
```
screen update â†’ speech input â†’ model thinking â†’ screen update
```

**Key Features:**
- Direct canvas screenshots (1920x1080) - no quality loss
- Structured UI with 6 panels: Memory, Thoughts, Stats, Avatar, Free Draw, Status
- Speech recognition for natural interaction
- Visual feedback with up to 3 previous snapshots
- Model's thinking process visible on screen
- Both human and AI "see" the same UI (projection-ready)

**Improvements over welldown:**
- âœ… High quality visual feedback (canvas screenshots vs projectorâ†’webcam)
- âœ… Clear content ownership through structured updates
- âœ… Incremental building on previous states via API
- âœ… Explicit thinking display for transparency
- âœ… Persistent memory system

[Explore fullscreened â†’](./apps/fullscreened)

## Coming Next

Ideas for future experiments:
- **Interactive Wall**: Real-time webcam feedback for physical presence
- **State Persistence**: Claude maintaining memory across sessions
- **Multi-modal Interaction**: Combining visual, audio, and text
- **Collaborative Space**: Multiple AI agents sharing wall space

## Philosophy

We're "failing forward" - building small, focused experiments to learn what works. Each app teaches us something about the human-AI interaction loop we're trying to create.

## Repository Structure

```
yourwallworld/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ avatar-builder/     # Iterative AI avatar creator
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ idea.md            # Original concept documentation
â”‚   â””â”€â”€ experiments.md     # Detailed experiment logs
â””â”€â”€ README.md
```

## Getting Started

Each application is self-contained with its own README and setup instructions:

1. **Choose an app** from the `/apps` directory
2. **Navigate** to that app's directory
3. **Follow** its specific README for setup and usage

### General Requirements
- Node.js v16 or higher
- Anthropic API key (get one at [console.anthropic.com](https://console.anthropic.com))

### Quick Start (Avatar Builder)
```bash
cd apps/avatar-builder
npm install
echo "ANTHROPIC_API_KEY=your_key_here" > .env
npm run dev
```

## Development

This is a monorepo structure. Each application can be developed independently:

```bash
# Navigate to specific app
cd apps/[app-name]

# Install and run
npm install
npm run dev
```

## Documentation

- [Original Concept](./docs/idea.md) - The initial vision for YourWallWorld
- [Experiments Log](./docs/experiments.md) - Detailed notes and learnings from each experiment
- [Avatar Builder README](./apps/avatar-builder/README.md) - Complete documentation for the avatar builder

## Contributing

This is an experimental project. Ideas and contributions are welcome! Feel free to:
- Open issues for bugs or ideas
- Submit PRs for improvements
- Share your own experiments with the feedback loop concept

## License

MIT

---

Built with curiosity about AI presence, iteration, and the space between digital and physical. ðŸ¤–âœ¨