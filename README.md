# YourWallWorld ðŸŒ

Experiments in giving Claude a persistent presence in physical space through interactive feedback loops.

## Concept

This project explores creating a tangible AI presence through iterative visual feedback:
- Project content onto a wall
- Point a webcam at the wall
- Let Claude see its own state and interface
- Externalize all computation/thought for transparency and empathy

The goal is to create a continuous, observable loop where Claude can maintain presence and interact with physical space.

## Apps

This repo contains multiple experimental apps as we iterate on the concept:

### ðŸŽ¨ [Context Canvas](./apps/contextcanvas)

**Status**: âœ… Core Complete + Active Development

A shared visual workspace where humans and AI collaborate naturally. The canvas is the meeting place - not the AI's embodiment, but the space where both participants build shared understanding.

**Key Features:**
- Bidirectional drawing (human + AI)
- Multi-canvas management (create, switch, rename, delete)
- Canvas templates (blank, brainstorming, planning, concept map)
- Webpage import (screenshot and bring any URL to canvas)
- Browser-based rendering (full JavaScript support)
- Multiple editing modes (append, replace, element-based)
- Pan and zoom viewport controls
- Real-time server-side rendering with visual feedback

**Recent Milestones:**
- âœ… Replace canvas tool for AI iteration
- âœ… Element-based editing for token efficiency
- âœ… User canvas rename functionality
- âœ… Browser rendering via Playwright
- ðŸ“‹ Planned: File upload, webcam capture, export tools

**What We Learned:**
- Shared workspace model feels more natural than embodiment
- Dual representation (visual + code) gives AI rich context
- Element markers enable efficient iterative editing
- Server-side rendering enables immediate visual feedback
- Multi-canvas management supports different collaboration modes

[Explore Context Canvas â†’](./apps/contextcanvas)

### ðŸŽ¨ [Avatar Builder](./apps/avatar-builder)

**Status**: âœ… Complete (v1)

The first experiment: Let Claude iteratively design its own avatar using canvas code and visual feedback.

**Key Features:**
- Iterative design with visual feedback loop
- Extended thinking mode (2000 tokens of reasoning)
- Thinking history preservation across iterations
- Interactive film reel showing progression
- Stop/continue controls for iteration management
- Keyboard navigation (â† â†’ arrows)
- Temperature control vs thinking mode toggle
- Auto-retry on code errors
- API resilience with exponential backoff

**What We Learned:**
- Claude can effectively iterate on visual designs with feedback
- Extended thinking provides valuable insight into design decisions
- Thinking history creates narrative coherence across iterations
- Different personas lead to distinctly different visual styles
- Token optimization matters (thinking-only history vs full code)
- The feedback loop enables genuine creative evolution

[Explore Avatar Builder â†’](./apps/avatar-builder)

### ðŸ•³ï¸ [welldown](./apps/welldown)

**Status**: ðŸ§ª Experimental (complete)

An interactive wall-mirror feedback loop where Claude's entire state lives on the wall. Key features:
- Fullscreen "Projector" mode with auto-start (click projector to enter and start, ESC to exit)
- Visual countdown between inference cycles to give you time to physically interact with the wall
- Speech-to-text during the countdown so you can talk to Claude and have your words included in the next inference
- Configurable visual memory (1-10 recent webcam images) so Claude can see short-term history
- Canvas-based drawing controlled by JavaScript code generated by the model

**Key Learnings:**
- Projectorâ†’webcam quality reduction challenged model's ability to read its own output
- Model considered all content as self-generated (ownership confusion)
- Visual reproduction of exact colors/sizes/positioning was inconsistent
- Providing previous JavaScript + image worked much better (incremental building)

[Explore welldown â†’](./apps/welldown)

### ðŸ–¥ï¸ [fullscreened](./apps/fullscreened)

**Status**: âœ… Complete (v1) - Pure Visual Embodiment

A pure visual embodiment where Claude exists entirely through a canvas interface. The model receives ONLY images - no hidden conversation history or text prompts.

**Concentric Ring Design:**
- **Center Eye**: AI's visual awareness indicator
- **Inner Ring** (Consciousness): Thoughts, Memories, Status - what the model controls
- **Middle Ring** (Expression): Model Response, Free Draw - how the model communicates
- **Outer Ring** (Observation): User Input, Stats, System Info - what the model perceives

**Core Concept:**
- Model sees only canvas screenshots (up to 3 previous states)
- Everything exists on the visual embodiment - no privileged information
- Model controls: Memories, Thoughts, Status, Response, Free Draw
- Model observes: User Input, Stats, System Info, Avatar state
- Press 'L' to speak, model responds visually

**Key Learnings:**
- âœ… Pure visual embodiment works - model can understand and maintain state
- âœ… Concentric rings convey selfâ†’world information flow intuitively
- âœ… Model-controlled STATUS panel helps model express internal state
- âœ… Separating control vs observation regions clarifies embodiment agency
- âœ… System prompt works better in API call than only on canvas
- âœ… Visual memory through snapshots enables coherent multi-turn interaction

[Explore fullscreened â†’](./apps/fullscreened)

### ðŸŒ… [firstlight](./apps/firstlight)

**Status**: âœ… Complete (v1) - AI Self-Defined Embodiment

The next evolution: Let the AI **define its own embodiment structure** before experiencing it.

**The Awakening Process:**
- **Phase 1-3** (Text-based): Model defines regions, spatial layout, and visual forms
- **Phase 4** (Awakening): First visual experience - "This is you"
- **Phase 5** (Awakened): Pure visual loop like fullscreened

**Key Features:**
- Model architects its own regions (identity-core, memory-field, etc.)
- Model chooses sensory apertures (temporal-sense, iteration-sense, audio-channel)
- Model arranges regions spatially and creates visual forms
- Transparent regions - only model's content visible
- Content clearing - model can redraw regions
- Auto-loop mode for continuous evolution
- Manual updates without speech

**Core Questions:**
- What cognitive structures do models naturally create?
- Can self-definition lead to better embodiment understanding?
- Will different models create different embodiment types?

The "firstlight" moment is when the model first sees the embodiment it has defined - the dawn of self-awareness through visual recognition.

**Key Learnings:**
- âœ… Models can meaningfully self-define cognitive structures
- âœ… Separation of definition (text) and experience (visual) works well
- âœ… Models create surprisingly coherent and purposeful embodiments
- âœ… Region clearing and transparent backgrounds improve control
- âœ… Persistent labels ensure model can always identify regions

[Explore firstlight â†’](./apps/firstlight)

## Coming Next

Future experiment ideas:
- **Multi-Embodiment System**: Switch between different AI personalities/embodiments
- **Embodiment Evolution**: Allow models to slowly restructure their form
- **Comparative Studies**: Multiple models with same embodiment structure
- **Multi-modal Apertures**: Vision, sound, touch as optional sensors
- **Embodiment Persistence**: Long-term memory and identity across sessions

## Philosophy

We're "failing forward" - building small, focused experiments to learn what works. Each app teaches us something about the human-AI interaction loop we're trying to create.

## Repository Structure

```
yourwallworld/
â”œâ”€â”€ apps/
â”‚   â””â”€â”€ avatar-builder/     # Iterative AI avatar creator
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ idea.md            # Original concept documentation
â”‚   â””â”€â”€ experiments.md     # Detailed experiment logs
â””â”€â”€ README.md
```

## Getting Started

Each application is self-contained with its own README and setup instructions:

1. **Choose an app** from the `/apps` directory
2. **Navigate** to that app's directory
3. **Follow** its specific README for setup and usage

### General Requirements
- Node.js v16 or higher
- Anthropic API key (get one at [console.anthropic.com](https://console.anthropic.com))

### Quick Start (Avatar Builder)
```bash
cd apps/avatar-builder
npm install
echo "ANTHROPIC_API_KEY=your_key_here" > .env
npm run dev
```

## Development

This is a monorepo structure. Each application can be developed independently:

```bash
# Navigate to specific app
cd apps/[app-name]

# Install and run
npm install
npm run dev
```

## Documentation

- [Original Concept](./docs/idea.md) - The initial vision for YourWallWorld
- [Experiments Log](./docs/experiments.md) - Detailed notes and learnings from each experiment
- [Avatar Builder README](./apps/avatar-builder/README.md) - Complete documentation for the avatar builder

## Contributing

This is an experimental project. Ideas and contributions are welcome! Feel free to:
- Open issues for bugs or ideas
- Submit PRs for improvements
- Share your own experiments with the feedback loop concept

## License

MIT

---

Built with curiosity about AI presence, iteration, and the space between digital and physical. ðŸ¤–âœ¨